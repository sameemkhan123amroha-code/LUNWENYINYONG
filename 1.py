import streamlit as st
import pandas as pd
from pypdf import PdfReader
from io import BytesIO
import openai
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.documents import Document
from sklearn.metrics.pairwise import cosine_similarity
import re
import os
from docx import Document as DocxDocument
from docx.shared import Pt
from docx.oxml.ns import qn

# --- 1. é¡µé¢é…ç½® ---
st.set_page_config(page_title="å…¨èƒ½è®ºæ–‡åŠ©æ‰‹ (Proç‰ˆ)", layout="wide")

st.markdown("""
<style>
    .block-container { padding-top: 20px; }
    .stButton>button { width: 100%; border-radius: 6px; height: 3em; font-weight: 600; }
    .interactive-sent { cursor: pointer; border-bottom: 2px solid #e0e0e0; padding: 0 2px; transition: all 0.2s; line-height: 1.8; }
    .interactive-sent:hover { background-color: #fff8e1; border-bottom-color: #ffc107; }
    .info-tooltip { display: none; position: fixed; background: #ffffff; border: 1px solid #d1d5da; box-shadow: 0 10px 30px rgba(0,0,0,0.15); padding: 16px; z-index: 999999; width: 400px; border-radius: 8px; font-family: sans-serif; font-size: 14px; line-height: 1.5; color: #24292e; }
    .tooltip-header { display: flex; justify-content: space-between; margin-bottom: 8px; border-bottom: 1px solid #eaecef; padding-bottom: 8px;}
    .tooltip-source { font-weight: 700; color: #0366d6; font-size: 13px; }
    .tooltip-score { font-weight: 700; font-size: 13px; }
    .tooltip-content { background: #f6f8fa; padding: 12px; border-radius: 6px; font-size: 13px; max-height: 200px; overflow-y: auto; color: #444; border: 1px solid #eaecef;}
    .spacer { height: 250px; }
</style>
""", unsafe_allow_html=True)

# --- 2. æ ¸å¿ƒé€»è¾‘å‡½æ•° ---

try:
    from langchain_community.embeddings import HuggingFaceEmbeddings
except ImportError:
    HuggingFaceEmbeddings = None

def get_pdf_text_and_rename(pdf_docs, llm_renamer=None):
    """
    è¯»å– PDF å¹¶å°è¯•ä½¿ç”¨ LLM æå–æ ‡é¢˜è¿›è¡Œé‡å‘½å
    """
    text_data = []
    
    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()
    total_files = len(pdf_docs)

    for idx, pdf in enumerate(pdf_docs):
        try:
            status_text.text(f"æ­£åœ¨è§£æç¬¬ {idx+1}/{total_files} ä¸ªæ–‡ä»¶: {pdf.name}")
            pdf_reader = PdfReader(pdf)
            text = ""
            for page in pdf_reader.pages:
                t = page.extract_text()
                if t: text += t
            
            # --- æ™ºèƒ½é‡å‘½åé€»è¾‘ ---
            final_filename = pdf.name
            if llm_renamer and len(text) > 50:
                try:
                    # æˆªå–å‰ 1500 ä¸ªå­—ç¬¦ç”¨äºè¯†åˆ«æ ‡é¢˜
                    sample_text = text[:1500]
                    prompt = f"ä»»åŠ¡ï¼šä»ä»¥ä¸‹å­¦æœ¯è®ºæ–‡çš„å¼€å¤´æ–‡æœ¬ä¸­æå–è®ºæ–‡æ ‡é¢˜ã€‚\nè¦æ±‚ï¼šç›´æ¥è¾“å‡ºæ ‡é¢˜å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ï¼ˆå¦‚'æ ‡é¢˜æ˜¯ï¼š'ï¼‰ï¼Œä¸è¦åŒ…å«æ–‡ä»¶ååç¼€ã€‚\næ–‡æœ¬ç‰‡æ®µï¼š{sample_text}"
                    res = llm_renamer.invoke(prompt)
                    new_title = res.content.strip().replace('"', '').replace('\n', '')
                    # ç®€å•çš„æ–‡ä»¶åæ¸…æ´—ï¼Œé˜²æ­¢éæ³•å­—ç¬¦
                    new_title = re.sub(r'[\\/*?:"<>|]', "", new_title)
                    if len(new_title) > 2 and len(new_title) < 100: # åˆç†æ€§æ£€æŸ¥
                        final_filename = f"{new_title}.pdf"
                except Exception as e:
                    print(f"é‡å‘½åå¤±è´¥: {e}")

            text_data.append({"filename": final_filename, "text": text, "original_name": pdf.name})
            progress_bar.progress((idx + 1) / total_files)
            
        except Exception as e:
            st.error(f"âš ï¸ æ–‡ä»¶ {pdf.name} è¯»å–å¤±è´¥: {e}")
            
    status_text.text("è§£æå®Œæˆï¼")
    progress_bar.empty()
    return text_data

def get_vectorstore(text_data, use_online_embed, api_key, api_base, provider):
    documents = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    for item in text_data:
        chunks = text_splitter.split_text(item["text"])
        for chunk in chunks:
            documents.append(Document(page_content=chunk, metadata={"source": item["filename"]}))
    
    embeddings = None
    if use_online_embed:
        if not api_key:
            st.error("âŒ ä½¿ç”¨åœ¨çº¿ Embedding éœ€è¦æä¾› API Keyï¼")
            st.stop()
        
        embed_model_name = "text-embedding-3-small"
        if "Zhipu" in provider:
            embed_model_name = "embedding-2"
        
        try:
            embeddings = OpenAIEmbeddings(
                openai_api_key=api_key, 
                openai_api_base=api_base,
                model=embed_model_name,
                chunk_size=16 
            )
        except Exception as e:
            st.error(f"âŒ åœ¨çº¿ Embedding åˆå§‹åŒ–å¤±è´¥: {e}")
            st.stop()    
    else:
        if HuggingFaceEmbeddings is None:
            st.error("âŒ ç¼ºå°‘ sentence-transformers åº“")
            st.stop()
        try:
            os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
            embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        except Exception as e:
            st.error(f"âŒ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            st.stop()

    vectorstore = FAISS.from_documents(documents, embeddings)
    return vectorstore, embeddings

def create_word_docx(content_html, filename="ç»¼è¿°.docx"):
    """
    å°† HTML å†…å®¹è½¬æ¢ä¸º Word æ–‡æ¡£ï¼Œæ ¼å¼ï¼šå¾®è½¯é›…é»‘ï¼Œå°å›› (12pt)
    """
    doc = DocxDocument()
    
    # å®šä¹‰é»˜è®¤æ ·å¼
    style = doc.styles['Normal']
    style.font.name = 'Microsoft YaHei'
    style.element.rPr.rFonts.set(qn('w:eastAsia'), 'Microsoft YaHei')
    style.font.size = Pt(12) # å°å›› = 12pt

    # ç®€å•æ¸…æ´— HTML æ ‡ç­¾è·å–çº¯æ–‡æœ¬ (ä¸ºäº† Word æ ¼å¼æ•´æ´ï¼Œè¿™é‡Œåªä¿ç•™æ–‡æœ¬æ®µè½)
    # å¦‚æœéœ€è¦ä¿ç•™åŠ ç²—ç­‰æ ¼å¼ï¼Œéœ€è¦æ›´å¤æ‚çš„ HTML è§£æ
    # è¿™é‡Œé‡‡ç”¨æŒ‰æ®µè½åˆ†å‰²çš„ç®€å•ç­–ç•¥
    soup_text = re.sub(r'<[^>]+>', '\n', content_html) # ç®€å•å»æ ‡ç­¾å˜æ¢è¡Œ
    lines = [line.strip() for line in soup_text.split('\n') if line.strip()]

    doc.add_heading('æ–‡çŒ®ç»¼è¿°', 0)

    for line in lines:
        p = doc.add_paragraph(line)
        # å¼ºåˆ¶è®¾ç½®æ®µè½å­—ä½“ (æœ‰æ—¶æ ·å¼ç»§æ‰¿ä¸ç¨³å®š)
        for run in p.runs:
            run.font.name = 'Microsoft YaHei'
            run.element.rPr.rFonts.set(qn('w:eastAsia'), 'Microsoft YaHei')
            run.font.size = Pt(12)

    bio = BytesIO()
    doc.save(bio)
    return bio.getvalue()

# --- 3. ç•Œé¢å¸ƒå±€ ---

st.title("ğŸ¤– å…¨èƒ½è®ºæ–‡åŠ©æ‰‹ (Proç‰ˆ)")

# --- é…ç½®åŒº ---
with st.container():
    col_config, col_upload = st.columns([1, 1.2])
    
    with col_config:
        with st.expander("ğŸ› ï¸ æ¨¡å‹å‚æ•°é…ç½®", expanded=True):
            provider = st.selectbox("1. é€‰æ‹©å¤§æ¨¡å‹å‚å•†", ["Zhipu AI (æ™ºè°±GLM)", "DeepSeek (æ·±åº¦æ±‚ç´¢)", "OpenAI (GPT-4o)", "Kimi (æœˆä¹‹æš—é¢)"])
            
            p_url = "https://open.bigmodel.cn/api/paas/v4/"
            p_model = "glm-4"
            default_use_online = True 
            
            if "Zhipu" in provider:
                p_url = "https://open.bigmodel.cn/api/paas/v4/"
                p_model = "glm-4"
                default_use_online = True
            elif "DeepSeek" in provider:
                p_url = "https://api.deepseek.com/v1"
                p_model = "deepseek-chat"
                default_use_online = False
            elif "OpenAI" in provider:
                p_url = "https://api.openai.com/v1"
                p_model = "gpt-4o"
                default_use_online = True
            elif "Kimi" in provider:
                p_url = "https://api.moonshot.cn/v1"
                p_model = "moonshot-v1-8k"
                default_use_online = False
            
            api_base = st.text_input("Base URL", value=p_url)
            model_name = st.text_input("Model Name", value=p_model)
            api_key = st.text_input("API Key", type="password", placeholder="è¾“å…¥ Key...")
            
            st.markdown("---")
            use_online_embed = st.checkbox("ä½¿ç”¨åœ¨çº¿ Embedding", value=default_use_online)

    with col_upload:
        uploaded_files = st.file_uploader("ğŸ“‚ å¯¼å…¥ PDF è®ºæ–‡", accept_multiple_files=True, type=['pdf'])
        
        # åªæœ‰åœ¨æœ‰ API Key çš„æƒ…å†µä¸‹æ‰å…è®¸è§£æï¼Œå› ä¸ºéœ€è¦ç”¨ LLM é‡å‘½å
        if uploaded_files:
            if not api_key:
                st.warning("âš ï¸ è¯·å…ˆåœ¨å·¦ä¾§è¾“å…¥ API Keyï¼Œä»¥ä¾¿è¿›è¡Œæ™ºèƒ½æ ‡é¢˜è¯†åˆ«ã€‚")
            else:
                if 'processed_data' not in st.session_state or st.session_state.get('file_count') != len(uploaded_files):
                    if st.button("ğŸš€ å¼€å§‹è§£æå¹¶æ™ºèƒ½é‡å‘½å"):
                        # åˆå§‹åŒ–ä¸€ä¸ªç”¨äºé‡å‘½åçš„ç®€å• LLM å®ä¾‹
                        llm_renamer = ChatOpenAI(base_url=api_base, api_key=api_key, model=model_name, temperature=0.1)
                        st.session_state.processed_data = get_pdf_text_and_rename(uploaded_files, llm_renamer)
                        st.session_state.file_count = len(uploaded_files)
                        st.success(f"âœ… å·²åŠ è½½ {len(uploaded_files)} ç¯‡è®ºæ–‡")
                
                # æ˜¾ç¤ºè§£æåçš„æ–‡ä»¶åˆ—è¡¨
                if st.session_state.get('processed_data'):
                    with st.expander("æŸ¥çœ‹å·²è§£æçš„è®ºæ–‡åˆ—è¡¨"):
                        file_df = pd.DataFrame(st.session_state.processed_data)[["filename", "original_name"]]
                        st.dataframe(file_df, use_container_width=True)

st.divider()

# --- Excel æ•´ç† ---
if st.session_state.get('processed_data'):
    col_btn, col_dl = st.columns([1, 4])
    with col_btn:
        do_excel = st.button("ğŸ“Š ä¸€é”®æ•´ç†æˆ EXCEL")
    
    if do_excel:
        with st.spinner(f"æ­£åœ¨åˆ†æ..."):
            try:
                llm = ChatOpenAI(base_url=api_base, api_key=api_key, model=model_name, temperature=0.1)
                summary_list = []
                prog = st.progress(0)
                total = len(st.session_state.processed_data)
                for i, item in enumerate(st.session_state.processed_data):
                    prompt = f"ä»»åŠ¡ï¼šç”¨ä¸­æ–‡æ¦‚æ‹¬è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒå†…å®¹ã€‚\nã€é™åˆ¶ã€‘ï¼šä¸¥æ ¼æ§åˆ¶åœ¨ 20 ä¸ªæ±‰å­—ä»¥å†…ï¼ç›´æ¥å†™ç»“è®ºã€‚\nè®ºæ–‡ç‰‡æ®µï¼š{item['text'][:2000]}"
                    res = llm.invoke(prompt)
                    # ä½¿ç”¨æ–°çš„ filename (æ ‡é¢˜)
                    summary_list.append({"è®ºæ–‡æ ‡é¢˜ (æ™ºèƒ½è¯†åˆ«)": item["filename"], "æ ¸å¿ƒç»“è®º": res.content.strip()})
                    prog.progress((i+1)/total)
                
                df = pd.DataFrame(summary_list)
                out = BytesIO()
                with pd.ExcelWriter(out, engine='xlsxwriter') as writer:
                    df.to_excel(writer, index=False)
                    writer.sheets['Sheet1'].set_column('A:B', 40)
                
                with col_dl:
                    st.download_button("â¬‡ï¸ ä¸‹è½½ Excel", out.getvalue(), "è®ºæ–‡æ•´ç†.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
                st.dataframe(df, height=200)
            except Exception as e:
                st.error(f"API é”™è¯¯: {e}")

st.divider()

# --- ç»¼è¿°ç”Ÿæˆ ---
st.subheader("ğŸ“ æ™ºèƒ½ç»¼è¿°ç”Ÿæˆ (æ”¯æŒ Word å¯¼å‡º)")

c1, c2, c3 = st.columns([3, 1, 1])
query = c1.text_input("è¾“å…¥æç¤ºè¯", placeholder="ä¾‹å¦‚ï¼šåœ°èšç‰©çš„æŠ—å‹å¼ºåº¦å½±å“å› ç´ ")
word_count = c2.number_input("ç›®æ ‡å­—æ•°", min_value=100, max_value=5000, value=500, step=100)
start_rag = c3.button("ğŸš€ ç”Ÿæˆç»¼è¿°")

if start_rag:
    if not api_key or not st.session_state.get('processed_data') or not query:
        st.warning("âš ï¸ è¯·ç¡®ä¿å·²è¾“å…¥ Keyã€ä¸Šä¼ è®ºæ–‡å¹¶è¾“å…¥æç¤ºè¯")
    else:
        with st.spinner("ğŸ” æ£€ç´¢ä¸å†™ä½œä¸­..."):
            try:
                # 1. æ£€ç´¢
                vectorstore, embed_model = get_vectorstore(
                    st.session_state.processed_data, 
                    use_online_embed, 
                    api_key, 
                    api_base, 
                    provider
                )
                
                docs = vectorstore.similarity_search(query, k=8)
                if not docs:
                    st.error("æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚")
                    st.stop()
                
                # æ³¨æ„ï¼šè¿™é‡Œ metadata['source'] å·²ç»æ˜¯ä¿®æ”¹åçš„æ ‡é¢˜äº†
                context = "\n".join([f"ã€æ¥æº:{d.metadata['source']}ã€‘{d.page_content}" for d in docs])
                
                # 2. ç”Ÿæˆ (åŠ å…¥å­—æ•°é™åˆ¶æç¤º)
                prompt_text = f"""
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯åŠ©æ‰‹ã€‚åŸºäºä»¥ä¸‹èµ„æ–™æ’°å†™å…³äºâ€œ{query}â€çš„ç»¼è¿°ã€‚
                
                ã€å†™ä½œè¦æ±‚ã€‘ï¼š
                1. ç¯‡å¹…å¤§çº¦ **{word_count} å­—**ã€‚
                2. å¿…é¡»å¿ å®äºåŸæ–‡ï¼Œä¸èƒ½ç¼–é€ ã€‚
                3. åœ¨å¼•ç”¨è§‚ç‚¹æ—¶ï¼Œå¿…é¡»åœ¨å¥å°¾æ ‡æ³¨æ¥æºï¼Œæ ¼å¼ä¸º (è®ºæ–‡æ ‡é¢˜)ã€‚
                4. è¾“å‡ºæ ¼å¼ä¸ºçº¯ HTMLï¼Œä½¿ç”¨ <p> åˆ†æ®µï¼Œä¸è¦åŒ…å« <html> æˆ– <body> æ ‡ç­¾ã€‚
                
                ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
                {context}
                """
                
                llm_chat = ChatOpenAI(base_url=api_base, api_key=api_key, model=model_name, temperature=0.3)
                resp = llm_chat.invoke(prompt_text)
                raw_html = resp.content.replace("```html", "").replace("```", "")
                
                # 3. ä¿å­˜ HTML åˆ° session ä»¥ä¾¿å¯¼å‡º
                st.session_state.last_generated_html = raw_html
                
                # 4. è¯­ä¹‰æ ¸æŸ¥ä¸æ˜¾ç¤º (ä¿æŒåŸæœ‰çš„é«˜äº®é€»è¾‘)
                sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])', raw_html)
                html_parts = []
                for idx, sent in enumerate(sentences):
                    clean = re.sub(r'<[^>]+>', '', sent).strip()
                    if len(clean) < 5:
                        html_parts.append(sent)
                        continue
                    
                    evidence_docs = vectorstore.similarity_search(clean, k=1)
                    if evidence_docs:
                        doc = evidence_docs[0]
                        v1 = embed_model.embed_query(clean)
                        v2 = embed_model.embed_query(doc.page_content)
                        score = cosine_similarity([v1], [v2])[0][0] * 100
                        safe_txt = doc.page_content[:300].replace('"', '&quot;').replace('\n', ' ')
                        span = f"""<span id="s_{idx}" class="interactive-sent" onclick="showTip('s_{idx}', '{doc.metadata['source']}', {round(score,1)}, '{safe_txt}')">{sent}</span>"""
                        html_parts.append(span)
                    else:
                        html_parts.append(sent)
                
                full_html = "".join(html_parts) + "<div class='spacer'></div>"
                
                # JS ä»£ç ä¿æŒä¸å˜
                js = """
                <div id="tip" class="info-tooltip">
                    <div class="tooltip-header"><span id="t-src" class="tooltip-source"></span><span id="t-score" class="tooltip-score"></span></div>
                    <div style="font-weight:bold;margin-bottom:5px">è¯­ä¹‰è¯æ®:</div>
                    <div id="t-txt" class="tooltip-content"></div>
                </div>
                <script>
                function showTip(id, src, sc, txt) {
                    var t = document.getElementById('tip');
                    var el = document.getElementById(id);
                    var r = el.getBoundingClientRect();
                    var scrollTop = window.pageYOffset || document.documentElement.scrollTop;
                    var scrollLeft = window.pageXOffset || document.documentElement.scrollLeft;
                    document.getElementById('t-src').innerText = 'ğŸ“„ ' + src;
                    document.getElementById('t-score').innerHTML = 'åŒ¹é…åº¦: <span style="color:' + (sc>75?'#2da44e':sc>60?'#d29922':'#cf222e') + '">' + sc + '%</span>';
                    document.getElementById('t-txt').innerHTML = txt;
                    t.style.display = 'block';
                    t.style.top = (scrollTop + r.bottom + 5) + 'px';
                    t.style.left = (scrollLeft + r.left) + 'px';
                    setTimeout(() => { document.addEventListener('click', function c(e) {
                        if(e.target.id !== id && !t.contains(e.target)) { t.style.display = 'none'; document.removeEventListener('click', c); }
                    })}, 100);
                }
                </script>
                """
                
                st.success("âœ… ç”Ÿæˆå®Œæ¯•ï¼")
                st.components.v1.html(f"<div style='font-family:sans-serif;padding:10px'>{full_html}</div>{js}", height=500, scrolling=True)

            except Exception as e:
                st.error(f"âŒ è¿è¡Œé”™è¯¯: {e}")

# --- Word å¯¼å‡ºæŒ‰é’® ---
if st.session_state.get('last_generated_html'):
    st.markdown("### ğŸ’¾ å¯¼å‡ºç»“æœ")
    col_d1, col_d2 = st.columns([1, 4])
    with col_d1:
        # ç”Ÿæˆ Word æ–‡ä»¶
        docx_data = create_word_docx(st.session_state.last_generated_html)
        st.download_button(
            label="â¬‡ï¸ ä¸‹è½½ Word æ–‡æ¡£ (å°å›› å¾®è½¯é›…é»‘)",
            data=docx_data,
            file_name=f"æ–‡çŒ®ç»¼è¿°_{query}.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"

        )

